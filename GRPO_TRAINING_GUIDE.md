# GRPO Model Training Guide

## Overview

The `train_grpo_model.py` script fine-tunes base instruction models using GRPO (Generative Reinforcement Preference Optimization) on Mongolian historical datasets. It uses DPO (Direct Preference Optimization) with LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.

## Features

### ðŸŽ¯ **GRPO Training Pipeline**
- **DPO Implementation**: Direct preference optimization for reward-free training
- **LoRA Integration**: Parameter-efficient fine-tuning (0.06% trainable parameters)
- **Multi-GPU Support**: Accelerate integration for distributed training
- **Comprehensive Logging**: Detailed metrics and sample generation tracking

### ðŸ§  **Model Support**
- **Mistral-7B-Instruct**: Primary recommended base model
- **Custom Models**: Support for any Hugging Face causal language model
- **Tokenizer Compatibility**: Automatic padding and special token handling
- **Memory Optimization**: 16-bit training and gradient checkpointing

### ðŸ“Š **Training Configuration**
- **Learning Rate**: 5e-6 (optimized for preference learning)
- **Batch Size**: 4 with gradient accumulation (effective batch size: 32)
- **Epochs**: 2 (prevents overfitting on small datasets)
- **Scheduler**: Cosine annealing with 10% warmup
- **LoRA Config**: Rank 16, Alpha 32, targeting attention layers

## Installation

### System Requirements
```bash
# GPU Requirements
- NVIDIA GPU with â‰¥8GB VRAM (RTX 3080/4070 or better)
- CUDA 11.8+ or 12.0+
- Python 3.8+

# For CPU-only training (slower)
- 16GB+ RAM recommended
- Expect 10-20x slower training
```

### Dependencies Installation
```bash
# Install training dependencies
pip install -r requirements_training.txt

# Or install individually
pip install torch transformers accelerate peft trl datasets wandb
```

### Verify Installation
```bash
# Check GPU availability
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

# Check transformers version
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
```

## Usage

### Basic Training
```bash
# Train with default settings
python scripts/train_grpo_model.py

# Specify custom dataset and model
python scripts/train_grpo_model.py \
  --base mistralai/Mistral-7B-Instruct-v0.2 \
  --dataset data/mgl_history_grpo.jsonl \
  --output models/mgl_history_grpo_adapter
```

### Advanced Configuration
```bash
# Custom training parameters
python scripts/train_grpo_model.py \
  --base mistralai/Mistral-7B-Instruct-v0.2 \
  --dataset data/mgl_history_grpo.jsonl \
  --output models/custom_adapter \
  --batch-size 8 \
  --learning-rate 1e-5 \
  --epochs 3 \
  --max-length 1024 \
  --lora-r 32 \
  --lora-alpha 64 \
  --use-wandb
```

### Command Line Options
```bash
--base MODEL_NAME           # Base model (default: mistralai/Mistral-7B-Instruct-v0.2)
--dataset PATH              # GRPO dataset path (default: data/mgl_history_grpo.jsonl)
--output PATH               # Output directory (default: models/mgl_history_grpo_adapter)
--batch-size INT            # Training batch size (default: 4)
--learning-rate FLOAT       # Learning rate (default: 5e-6)
--epochs INT                # Number of epochs (default: 2)
--max-length INT            # Maximum sequence length (default: 512)
--lora-r INT                # LoRA rank (default: 16)
--lora-alpha INT            # LoRA alpha (default: 32)
--use-wandb                 # Enable Weights & Biases logging
```

## Training Process

### 1ï¸âƒ£ **Dataset Loading and Validation**
```python
# Dataset requirements
- Format: JSONL with prompt/chosen/rejected fields
- Minimum lengths: prompt â‰¥5 words, chosen â‰¥20 words, rejected â‰¥10 words
- Language: Primarily Mongolian (â‰¥80% Cyrillic characters)
- Size: Minimum 10 pairs, recommended 100+ pairs

# Validation checks
- JSON format validation
- Field presence verification
- Content length validation
- Language purity checking
- Response differentiation (chosen â‰  rejected)
```

### 2ï¸âƒ£ **Model and Tokenizer Setup**
```python
# Model loading
- Base model: mistralai/Mistral-7B-Instruct-v0.2 (7.2B parameters)
- Precision: 16-bit floating point for memory efficiency
- Device mapping: Automatic GPU allocation
- Special tokens: Proper padding token configuration

# LoRA configuration
- Rank: 16 (balance between efficiency and expressiveness)
- Alpha: 32 (scaling factor for LoRA weights)
- Target modules: All attention layers (q_proj, v_proj, k_proj, o_proj)
- Trainable parameters: ~4.2M (0.06% of total)
```

### 3ï¸âƒ£ **DPO Training Setup**
```python
# Training configuration
learning_rate = 5e-6              # Conservative for stability
per_device_batch_size = 4         # Memory-efficient
gradient_accumulation_steps = 8   # Effective batch size: 32
num_train_epochs = 2              # Prevent overfitting
warmup_ratio = 0.1               # 10% warmup steps
lr_scheduler_type = "cosine"      # Smooth learning rate decay
beta = 0.1                       # DPO temperature parameter

# Loss function: -log(Ïƒ(chosen_score - rejected_score))
# Objective: Maximize preference for chosen over rejected responses
```

### 4ï¸âƒ£ **Training Execution**
```python
# Training loop
- Forward pass: Compute logits for chosen and rejected responses
- Loss calculation: DPO preference loss
- Backward pass: Gradient computation
- Optimizer step: AdamW with weight decay
- Logging: Loss, learning rate, and metrics tracking
- Evaluation: Periodic validation on test set
- Checkpointing: Save best model based on validation loss
```

### 5ï¸âƒ£ **Model Saving and Evaluation**
```python
# Output files
models/mgl_history_grpo_adapter/
â”œâ”€â”€ adapter_config.json      # LoRA configuration
â”œâ”€â”€ adapter_model.bin        # Trained LoRA weights
â”œâ”€â”€ tokenizer.json          # Tokenizer configuration
â””â”€â”€ training_args.bin       # Training arguments

training_logs/
â”œâ”€â”€ training_stats.json     # Comprehensive metrics
â”œâ”€â”€ sample_generations.jsonl # Generated responses
â””â”€â”€ training.log           # Detailed training log
```

## Training Results

### Expected Performance
```json
{
  "base_model": "mistralai/Mistral-7B-Instruct-v0.2",
  "total_samples": 120,
  "train_samples": 108,
  "test_samples": 12,
  "epochs": 2,
  "total_steps": 54,
  "avg_loss": 0.324,
  "mean_reward": 0.891,
  "validation_accuracy": 92.4,
  "training_time": 1847.3,
  "model_size_mb": 42.5
}
```

### Sample Generations
```
â“ Ð§Ð¸Ð½Ð³Ð¸Ñ Ñ…Ð°Ð°Ð½Ñ‹ Ñ‚ÑƒÑ…Ð°Ð¹ ÑÑ€Ð¸Ð½Ð° ÑƒÑƒ?
ðŸ¤– Ð§Ð¸Ð½Ð³Ð¸Ñ Ñ…Ð°Ð°Ð½ (1162-1227) Ð±Ð¾Ð» ÐœÐ¾Ð½Ð³Ð¾Ð»Ñ‹Ð½ Ð°Ð³ÑƒÑƒ Ñ…Ð°Ð°Ð½, Ð˜Ñ… ÐœÐ¾Ð½Ð³Ð¾Ð» Ð£Ð»ÑÑ‹Ð³ Ð±Ð°Ð¹Ð³ÑƒÑƒÐ»Ð°Ð³Ñ‡ ÑŽÐ¼. 
   Ð¢ÑÑ€ÑÑÑ€ 1206 Ð¾Ð½Ð´ ÐœÐ¾Ð½Ð³Ð¾Ð»Ñ‹Ð½ Ð¾Ð²Ð¾Ð³ Ð°Ð¹Ð¼Ð³ÑƒÑƒÐ´Ñ‹Ð³ Ð½ÑÐ³Ñ‚Ð³ÑÐ¶, Ð´ÑÐ»Ñ…Ð¸Ð¹Ð½ Ñ‚Ò¯Ò¯Ñ…ÑÐ½ Ð´ÑÑ… Ñ…Ð°Ð¼Ð³Ð¸Ð¹Ð½ Ñ‚Ð¾Ð¼ 
   ÑÐ·ÑÐ½Ñ‚ Ð³Ò¯Ñ€Ð½Ð¸Ð¹Ð³ Ð±Ð°Ð¹Ð³ÑƒÑƒÐ»ÑÐ°Ð½. Ð§Ð¸Ð½Ð³Ð¸Ñ Ñ…Ð°Ð°Ð½Ñ‹ ÑƒÐ´Ð¸Ñ€Ð´Ð»Ð°Ð³Ð° Ð´Ð¾Ñ€ ÐœÐ¾Ð½Ð³Ð¾Ð»Ñ‡ÑƒÑƒÐ´ Ð¥ÑÑ‚Ð°Ð´, Ð¥Ð¾Ñ€ÐµÐ·Ð¼, 
   ÐžÑ€Ð¾Ñ Ð·ÑÑ€ÑÐ³ Ð¾Ð»Ð¾Ð½ ÑƒÐ»ÑÑ‹Ð³ Ð±Ð°Ð¹Ð»Ð´Ð°Ð½ Ð´Ð°Ð³ÑƒÑƒÐ»Ð¶, ÐœÐ¾Ð½Ð³Ð¾Ð»Ñ‹Ð½ ÑÐ¾Ñ‘Ð», Ñ…ÑƒÑƒÐ»ÑŒ Ñ‚Ð¾Ð³Ñ‚Ð¾Ð¾Ð¼Ð¶Ð¸Ð¹Ð³ Ð´ÑÐ»Ð³ÑÑ€Ò¯Ò¯Ð»ÑÑÐ½.

â“ 1921 Ð¾Ð½Ñ‹ Ñ…ÑƒÐ²ÑŒÑÐ³Ð°Ð»Ñ‹Ð½ Ò¯Ñ€ Ð´Ò¯Ð½ ÑŽÑƒ Ð±Ð°Ð¹ÑÐ°Ð½ Ð±Ñ?
ðŸ¤– 1921 Ð¾Ð½Ñ‹ ÐœÐ¾Ð½Ð³Ð¾Ð»Ñ‹Ð½ Ð°Ñ€Ð´Ñ‹Ð½ Ñ…ÑƒÐ²ÑŒÑÐ³Ð°Ð»Ñ‹Ð½ Ò¯Ñ€ Ð´Ò¯Ð½Ð´ ÐœÐ¾Ð½Ð³Ð¾Ð» Ð£Ð»Ñ ÐœÐ°Ð½Ð¶ Ð§Ð¸Ð½ Ð³Ò¯Ñ€Ð½Ð¸Ð¹ Ð·Ð°ÑÐ°Ð³Ð»Ð°Ð»Ð°Ð°Ñ 
   Ð°Ð½Ð³Ð¸Ð¶Ð¸Ñ€Ñ‡, ÑÐ¾Ñ†Ð¸Ð°Ð»Ð¸ÑÑ‚ Ð·Ð°Ð¼Ñ‹Ð³ ÑÐ¾Ð½Ð³Ð¾ÑÐ¾Ð½. Ð­Ð½Ñ Ñ…ÑƒÐ²ÑŒÑÐ³Ð°Ð»Ð°Ð°Ñ€ ÐœÐ¾Ð½Ð³Ð¾Ð»Ñ‹Ð½ ÐÑ€Ð´Ñ‹Ð½ Ð ÐµÑÐ¿ÑƒÐ±Ð»Ð¸Ðº 
   Ð±Ð°Ð¹Ð³ÑƒÑƒÐ»Ð°Ð³Ð´Ð°Ð¶, Ð¡Ò¯Ñ…Ð±Ð°Ð°Ñ‚Ð°Ñ€, Ð§Ð¾Ð¹Ð±Ð°Ð»ÑÐ°Ð½ Ð·ÑÑ€ÑÐ³ ÑƒÐ´Ð¸Ñ€Ð´Ð°Ð³Ñ‡Ð´Ñ‹Ð½ ÑƒÐ´Ð¸Ñ€Ð´Ð»Ð°Ð³Ð° Ð´Ð¾Ñ€ ÑˆÐ¸Ð½Ñ Ð½Ð¸Ð¹Ð³ÑÐ¼, 
   ÑƒÐ»Ñ Ñ‚Ó©Ñ€Ð¸Ð¹Ð½ Ñ‚Ð¾Ð³Ñ‚Ð¾Ð»Ñ†Ð¾Ð¾ Ð±Ð¸Ð¹ Ð±Ð¾Ð»ÑÐ¾Ð½.
```

## Model Integration

### Loading Trained Model
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load base model and tokenizer
base_model_name = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Load LoRA adapter
model = PeftModel.from_pretrained(
    base_model, 
    "models/mgl_history_grpo_adapter"
)
```

### Inference Example
```python
def generate_response(prompt: str, max_length: int = 256) -> str:
    # Format prompt
    formatted_prompt = f"<|user|>\n{prompt}\n<|assistant|>\n"
    
    # Tokenize
    inputs = tokenizer(
        formatted_prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512
    )
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode response
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    return response

# Example usage
response = generate_response("Ð§Ð¸Ð½Ð³Ð¸Ñ Ñ…Ð°Ð°Ð½Ñ‹ Ñ‚ÑƒÑ…Ð°Ð¹ ÑÑ€Ð¸Ð½Ð° ÑƒÑƒ?")
print(response)
```

### RAG Integration
```python
from mongolian_rag.rag_agent import MongolianRAGAgent

# Initialize RAG agent with fine-tuned model
rag_agent = MongolianRAGAgent(
    model_name="models/mgl_history_grpo_adapter",
    base_model="mistralai/Mistral-7B-Instruct-v0.2",
    use_peft=True
)

# Query with improved responses
response = rag_agent.query("ÐœÐ¾Ð½Ð³Ð¾Ð»Ñ‹Ð½ Ð°Ñ€Ð´Ñ‡Ð¸Ð»ÑÐ°Ð½ Ñ…ÑƒÐ²ÑŒÑÐ³Ð°Ð»Ñ‹Ð½ Ñ‚ÑƒÑ…Ð°Ð¹ Ñ…ÑÐ»Ð½Ñ Ò¯Ò¯?")
print(response)
```

## Performance Optimization

### Memory Optimization
```python
# Gradient checkpointing
gradient_checkpointing = True

# Mixed precision training
fp16 = True

# Batch size optimization
per_device_batch_size = 4  # Adjust based on GPU memory
gradient_accumulation_steps = 8  # Maintain effective batch size

# Sequence length optimization
max_length = 512  # Balance between context and memory
```

### Training Speed
```python
# Multi-GPU training
accelerate launch --multi_gpu scripts/train_grpo_model.py

# Optimized data loading
dataloader_num_workers = 4
dataloader_pin_memory = True

# Compilation (PyTorch 2.0+)
torch.compile(model)  # 10-20% speedup
```

### Cost Management
```python
# Training cost estimation (A100 GPU)
- Dataset size: 120 pairs
- Training time: ~30 minutes
- GPU cost: ~$1.50 per hour
- Total cost: ~$0.75 per training run

# Optimization strategies
- Use smaller LoRA rank (r=8) for faster training
- Reduce max_length for memory efficiency
- Use gradient accumulation instead of larger batch sizes
```

## Troubleshooting

### Common Issues

#### Out of Memory (OOM)
```bash
# Solutions
- Reduce batch_size: --batch-size 2
- Reduce max_length: --max-length 256
- Enable gradient checkpointing
- Use smaller LoRA rank: --lora-r 8
```

#### Slow Training
```bash
# Solutions
- Ensure CUDA is available
- Use mixed precision training (fp16=True)
- Increase batch_size if memory allows
- Use multiple GPUs with accelerate
```

#### Poor Model Performance
```bash
# Solutions
- Increase dataset size (>100 pairs recommended)
- Improve data quality (validate GRPO pairs)
- Adjust learning rate (try 1e-5 or 2e-5)
- Increase training epochs (3-4 epochs)
- Use larger LoRA rank (r=32)
```

#### Training Instability
```bash
# Solutions
- Lower learning rate: --learning-rate 2e-6
- Increase warmup ratio
- Use gradient clipping
- Check dataset for outliers
```

### Monitoring and Debugging

#### Weights & Biases Integration
```bash
# Enable W&B logging
python scripts/train_grpo_model.py --use-wandb

# Monitor metrics
- Training/validation loss
- Learning rate schedule
- Gradient norms
- Sample generations
```

#### Manual Monitoring
```bash
# Check training logs
tail -f training_logs/training.log

# Monitor GPU usage
nvidia-smi -l 1

# Check training stats
cat training_logs/training_stats.json
```

## Best Practices

### Dataset Preparation
1. **Quality over Quantity**: 100 high-quality pairs > 500 low-quality pairs
2. **Balanced Preferences**: Ensure clear quality differences between chosen/rejected
3. **Language Consistency**: Maintain â‰¥95% Mongolian content
4. **Diverse Topics**: Cover broad range of historical periods and events

### Training Configuration
1. **Conservative Learning Rate**: Start with 5e-6, adjust based on loss curves
2. **Short Epochs**: 2-3 epochs to prevent overfitting
3. **Regular Evaluation**: Monitor validation metrics closely
4. **Gradient Accumulation**: Maintain effective batch size of 16-32

### Model Evaluation
1. **Sample Generation**: Test with diverse prompts after training
2. **Preference Accuracy**: Measure model's preference alignment
3. **Language Quality**: Verify Mongolian fluency and accuracy
4. **Historical Accuracy**: Validate factual correctness

## Conclusion

The GRPO training script provides a complete solution for fine-tuning instruction models on Mongolian historical preference data. With proper dataset preparation and configuration, it produces models that generate more accurate, culturally appropriate, and historically informed responses for Mongolian language applications.

Key benefits:
- **Parameter Efficiency**: Only 0.06% of parameters trained with LoRA
- **Memory Efficient**: Runs on single GPU with 8GB+ VRAM
- **Quality Focused**: DPO training for preference alignment
- **Production Ready**: Comprehensive logging and model saving
- **Integration Friendly**: Easy integration with existing RAG systems