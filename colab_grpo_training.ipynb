{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mongolian-grpo-training"
   },
   "source": [
    "# üöÄ Mongolian GRPO Model Training on Google Colab\n",
    "\n",
    "This notebook runs the complete GRPO training pipeline on Google Colab with GPU acceleration.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro (recommended for T4/V100 GPU)\n",
    "- OpenAI API key for dataset generation\n",
    "- Runtime: GPU (T4, V100, or A100)\n",
    "\n",
    "**Expected Training Time:** 30-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-environment"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Please enable GPU runtime:\")\n",
    "    print(\"Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install training dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers>=4.35.0\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q peft>=0.6.0\n",
    "!pip install -q trl>=0.7.0\n",
    "!pip install -q datasets>=2.14.0\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "!pip install -q wandb>=0.15.0\n",
    "!pip install -q openai>=1.0.0\n",
    "!pip install -q tqdm numpy scipy\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Upload Project Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-project"
   },
   "outputs": [],
   "source": [
    "# Option 1: Upload from local files\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"üì§ Upload your project as a ZIP file:\")\n",
    "print(\"1. Zip your entire project folder\")\n",
    "print(\"2. Upload it using the file browser below\")\n",
    "print(\"3. The notebook will extract it automatically\")\n",
    "\n",
    "# Upload ZIP file\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract the first ZIP file found\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f\"üì¶ Extracting {filename}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(\"‚úÖ Project extracted successfully!\")\n",
    "        break\n",
    "\n",
    "# List project structure\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alternative-setup"
   },
   "outputs": [],
   "source": [
    "# Option 2: Clone from GitHub (if you've pushed your project)\n",
    "# Uncomment and modify the following lines:\n",
    "\n",
    "# !git clone https://github.com/yourusername/mongolian-history-project.git\n",
    "# %cd mongolian-history-project\n",
    "# !ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set-api-keys"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set OpenAI API key\n",
    "print(\"üîë Enter your OpenAI API key:\")\n",
    "openai_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ['OPENAI_API_KEY'] = openai_key\n",
    "\n",
    "# Optional: Set Weights & Biases key for experiment tracking\n",
    "print(\"\\nüîë Enter your W&B API key (optional, press Enter to skip):\")\n",
    "wandb_key = getpass(\"W&B API Key (optional): \")\n",
    "if wandb_key:\n",
    "    os.environ['WANDB_API_KEY'] = wandb_key\n",
    "    !wandb login\n",
    "\n",
    "print(\"‚úÖ API keys configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Generate GRPO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-grpo-dataset"
   },
   "outputs": [],
   "source": [
    "# Check if we have existing datasets\n",
    "!ls -la data/\n",
    "\n",
    "# Generate GRPO dataset if not exists\n",
    "import os\n",
    "if not os.path.exists('data/mgl_history_grpo.jsonl'):\n",
    "    print(\"üìä Generating GRPO dataset...\")\n",
    "    !python scripts/build_grpo_dataset.py --pairs-per-topic 20 --output data/mgl_history_grpo.jsonl\n",
    "else:\n",
    "    print(\"‚úÖ GRPO dataset already exists\")\n",
    "\n",
    "# Validate the dataset\n",
    "!python scripts/validate_mgl_dataset.py --files data/mgl_history_grpo.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-grpo-training"
   },
   "outputs": [],
   "source": [
    "# Start GRPO training with optimized settings for Colab\n",
    "training_command = \"\"\"\n",
    "python scripts/train_grpo_model.py \\\n",
    "  --base mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "  --dataset data/mgl_history_grpo.jsonl \\\n",
    "  --output models/mgl_history_grpo_adapter \\\n",
    "  --batch-size 2 \\\n",
    "  --learning-rate 5e-6 \\\n",
    "  --epochs 2 \\\n",
    "  --max-length 512 \\\n",
    "  --lora-r 16 \\\n",
    "  --lora-alpha 32\n",
    "\"\"\"\n",
    "\n",
    "# Add W&B logging if key is set\n",
    "if 'WANDB_API_KEY' in os.environ:\n",
    "    training_command += \" --use-wandb\"\n",
    "\n",
    "print(\"üî• Starting GRPO training...\")\n",
    "print(f\"Command: {training_command.strip()}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Execute training\n",
    "!{training_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "monitor-training"
   },
   "outputs": [],
   "source": [
    "# Check training logs\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display training statistics\n",
    "if os.path.exists('training_logs/training_stats.json'):\n",
    "    with open('training_logs/training_stats.json', 'r') as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    print(\"üìä TRAINING RESULTS\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Base model: {stats.get('base_model', 'N/A')}\")\n",
    "    print(f\"Dataset samples: {stats.get('total_samples', 'N/A')}\")\n",
    "    print(f\"Training samples: {stats.get('train_samples', 'N/A')}\")\n",
    "    print(f\"Test samples: {stats.get('test_samples', 'N/A')}\")\n",
    "    print(f\"Epochs: {stats.get('epochs', 'N/A')}\")\n",
    "    print(f\"Total steps: {stats.get('total_steps', 'N/A')}\")\n",
    "    print(f\"Average loss: {stats.get('avg_loss', 'N/A'):.4f}\")\n",
    "    print(f\"Final loss: {stats.get('final_loss', 'N/A'):.4f}\")\n",
    "    print(f\"Mean reward: {stats.get('mean_reward', 'N/A'):.3f}\")\n",
    "    print(f\"Validation accuracy: {stats.get('validation_accuracy', 'N/A'):.1%}\")\n",
    "    print(f\"Training time: {stats.get('training_time', 'N/A'):.1f}s\")\n",
    "    print(f\"Model size: {stats.get('model_size_mb', 'N/A'):.1f}MB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training statistics not found. Check if training completed successfully.\")\n",
    "\n",
    "# Check model files\n",
    "print(\"\\nüìÅ MODEL FILES:\")\n",
    "!ls -la models/mgl_history_grpo_adapter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-trained-model"
   },
   "outputs": [],
   "source": [
    "# Load and test the trained model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "def load_trained_model():\n",
    "    \"\"\"Load the trained GRPO model.\"\"\"\n",
    "    base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    adapter_path = \"models/mgl_history_grpo_adapter\"\n",
    "    \n",
    "    print(\"üì• Loading base model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"üîß Loading LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_length=256):\n",
    "    \"\"\"Generate response using the trained model.\"\"\"\n",
    "    formatted_prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Load the trained model\n",
    "if os.path.exists('models/mgl_history_grpo_adapter'):\n",
    "    try:\n",
    "        model, tokenizer = load_trained_model()\n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "        \n",
    "        # Test with sample prompts\n",
    "        test_prompts = [\n",
    "            \"–ß–∏–Ω–≥–∏—Å —Ö–∞–∞–Ω—ã —Ç—É—Ö–∞–π —è—Ä–∏–Ω–∞ —É—É?\",\n",
    "            \"1921 –æ–Ω—ã —Ö—É–≤—å—Å–≥–∞–ª—ã–Ω “Ø—Ä –¥“Ø–Ω —é—É –±–∞–π—Å–∞–Ω –±—ç?\",\n",
    "            \"–ú–æ–Ω–≥–æ–ª—ã–Ω –∞—Ä–¥—á–∏–ª—Å–∞–Ω —Ö—É–≤—å—Å–≥–∞–ª —Ö—ç—Ä—Ö—ç–Ω ”©—Ä–Ω”©—Å”©–Ω –±—ç?\",\n",
    "            \"–ë–æ–≥–¥ —Ö–∞–∞–Ω—ã “Ø–µ–∏–π–Ω –æ–Ω—Ü–ª–æ–≥ –Ω—å —é—É –≤—ç?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüß™ TESTING TRAINED MODEL\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for i, prompt in enumerate(test_prompts, 1):\n",
    "            print(f\"\\n{i}. ‚ùì {prompt}\")\n",
    "            response = generate_response(model, tokenizer, prompt)\n",
    "            print(f\"   ü§ñ {response}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        print(\"This might be due to memory constraints or incomplete training.\")\nelse:\n",
    "    print(\"‚ö†Ô∏è Trained model not found. Make sure training completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-model"
   },
   "outputs": [],
   "source": [
    "# Create a ZIP file with the trained model and logs\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_model_archive():\n",
    "    \"\"\"Create a ZIP archive with trained model and logs.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"mongolian_grpo_model_{timestamp}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add model files\n",
    "        if os.path.exists('models/mgl_history_grpo_adapter'):\n",
    "            for root, dirs, files in os.walk('models/mgl_history_grpo_adapter'):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, '.')\n",
    "                    zipf.write(file_path, arcname)\n",
    "        \n",
    "        # Add training logs\n",
    "        if os.path.exists('training_logs'):\n",
    "            for root, dirs, files in os.walk('training_logs'):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, '.')\n",
    "                    zipf.write(file_path, arcname)\n",
    "        \n",
    "        # Add dataset if exists\n",
    "        if os.path.exists('data/mgl_history_grpo.jsonl'):\n",
    "            zipf.write('data/mgl_history_grpo.jsonl', 'data/mgl_history_grpo.jsonl')\n",
    "    \n",
    "    return archive_name\n",
    "\n",
    "# Create and download the archive\n",
    "if os.path.exists('models/mgl_history_grpo_adapter'):\n",
    "    print(\"üì¶ Creating model archive...\")\n",
    "    archive_name = create_model_archive()\n",
    "    \n",
    "    print(f\"‚úÖ Archive created: {archive_name}\")\n",
    "    print(f\"üìÅ Archive size: {os.path.getsize(archive_name) / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Download the archive\n",
    "    print(\"‚¨áÔ∏è Downloading archive...\")\n",
    "    files.download(archive_name)\n",
    "    \n",
    "    print(\"\\nüéâ SUCCESS! Your trained model is ready!\")\n",
    "    print(\"\\nüìã What you got:\")\n",
    "    print(\"‚úÖ Trained LoRA adapter (models/mgl_history_grpo_adapter/)\")\n",
    "    print(\"‚úÖ Training logs and statistics (training_logs/)\")\n",
    "    print(\"‚úÖ GRPO dataset (data/mgl_history_grpo.jsonl)\")\n",
    "    \n",
    "    print(\"\\nüöÄ Next steps:\")\n",
    "    print(\"1. Extract the ZIP file in your local project\")\n",
    "    print(\"2. Load the model with: PeftModel.from_pretrained(base_model, 'models/mgl_history_grpo_adapter')\")\n",
    "    print(\"3. Integrate with your RAG system for improved Mongolian responses\")\n",
    "    \nelse:\n",
    "    print(\"‚ö†Ô∏è No trained model found to download.\")\n",
    "    print(\"Make sure the training completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-summary"
   },
   "outputs": [],
   "source": [
    "# Display final training summary\n",
    "print(\"üéâ MONGOLIAN GRPO TRAINING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check GPU usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üî• GPU Used: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ VRAM Used: {torch.cuda.memory_allocated(0) / 1024**3:.1f} GB\")\n",
    "\n",
    "# Check training results\n",
    "if os.path.exists('training_logs/training_stats.json'):\n",
    "    with open('training_logs/training_stats.json', 'r') as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    print(f\"\\nüìà Training Results:\")\n",
    "    print(f\"   ‚Ä¢ Final Loss: {stats.get('final_loss', 'N/A'):.4f}\")\n",
    "    print(f\"   ‚Ä¢ Validation Accuracy: {stats.get('validation_accuracy', 'N/A'):.1%}\")\n",
    "    print(f\"   ‚Ä¢ Mean Reward: {stats.get('mean_reward', 'N/A'):.3f}\")\n",
    "    print(f\"   ‚Ä¢ Training Time: {stats.get('training_time', 'N/A'):.1f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Your Mongolian historical AI model is now trained and ready!\")\n",
    "print(\"\\nüîÆ The model can now provide more accurate, culturally appropriate,\")\n",
    "print(\"   and historically informed responses in Mongolian language.\")\n",
    "\n",
    "print(\"\\nüéØ Integration Example:\")\n",
    "print(\"```python\")\n",
    "print(\"from peft import PeftModel\")\n",
    "print(\"from transformers import AutoModelForCausalLM\")\n",
    "print(\"\")\n",
    "print(\"base_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2')\")\n",
    "print(\"model = PeftModel.from_pretrained(base_model, 'models/mgl_history_grpo_adapter')\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\nüåü Congratulations on successfully training your Mongolian AI model!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}