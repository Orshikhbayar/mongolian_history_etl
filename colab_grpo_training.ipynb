{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "mongolian-grpo-training"
            },
            "source": [
                "# üöÄ Mongolian GRPO Model Training on Google Colab\n",
                "\n",
                "This notebook runs the complete GRPO training pipeline on Google Colab with GPU acceleration.\n",
                "\n",
                "**Requirements:**\n",
                "- Google Colab Pro (recommended for T4/V100 GPU)\n",
                "- OpenAI API key for dataset generation\n",
                "- Runtime: GPU (T4, V100, or A100)\n",
                "\n",
                "**Expected Training Time:** 30-60 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup-environment"
            },
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "import torch\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No GPU detected. Please enable GPU runtime:\")\n",
                "    print(\"Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install-dependencies"
            },
            "outputs": [],
            "source": [
                "# Install training dependencies\n",
                "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "!pip install -q transformers>=4.35.0\n",
                "!pip install -q accelerate>=0.24.0\n",
                "!pip install -q peft>=0.6.0\n",
                "!pip install -q trl>=0.7.0\n",
                "!pip install -q datasets>=2.14.0\n",
                "!pip install -q bitsandbytes>=0.41.0\n",
                "!pip install -q wandb>=0.15.0\n",
                "!pip install -q openai>=1.0.0\n",
                "!pip install -q tqdm numpy scipy\n",
                "\n",
                "print(\"‚úÖ Dependencies installed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß Setup Environment and Clone Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "setup-clone-repo"
            },
            "outputs": [],
            "source": [
                "#@title üîß Setup Environment and Clone from GitHub\n",
                "#@markdown This cell will:\n",
                "#@markdown - Check GPU availability\n",
                "#@markdown - Fix dependency conflicts (pyarrow)\n",
                "#@markdown - Install all required dependencies\n",
                "#@markdown - **Clone repository from GitHub** (not upload)\n",
                "#@markdown - Verify the stable GRPO dataset exists\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import torch\n",
                "\n",
                "# Check GPU\n",
                "print(\"üîç Checking GPU availability...\")\n",
                "!nvidia-smi\n",
                "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No GPU detected. Please enable GPU runtime:\")\n",
                "    print(\"Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
                "\n",
                "# Fix pyarrow dependency conflict\n",
                "# Note: cudf requires pyarrow<20.0, but we don't need cudf for GRPO training\n",
                "# This fix prevents warnings and ensures compatibility\n",
                "print(\"\\nüîß Fixing dependency conflicts...\")\n",
                "print(\"   (cudf requires pyarrow<20.0, but this won't affect GRPO training)\")\n",
                "!pip install \"pyarrow>=14.0.0,<20.0.0\" --force-reinstall --no-deps 2>/dev/null || true\n",
                "!pip install pyarrow --no-deps 2>/dev/null || true\n",
                "print(\"‚úÖ Dependency conflicts resolved (safe to ignore any remaining warnings)\")\n",
                "\n",
                "# Install dependencies optimized for GRPO training\n",
                "print(\"\\nüì¶ Installing training dependencies...\")\n",
                "!pip install -q transformers>=4.35.0 trl>=0.7.0 accelerate>=0.24.0 peft>=0.6.0 datasets>=2.14.0 bitsandbytes>=0.41.0 --upgrade\n",
                "!pip install -q wandb openai tqdm numpy scipy matplotlib\n",
                "print(\"‚úÖ Dependencies installed\")\n",
                "\n",
                "# Clone repository from GitHub\n",
                "print(\"\\nüì• Cloning repository from GitHub...\")\n",
                "print(\"   Repository: https://github.com/Orshikhbayar/mongolian_history_etl.git\")\n",
                "\n",
                "repo_path = \"/content/mongolian_history_etl\"\n",
                "repo_url = \"https://github.com/Orshikhbayar/mongolian_history_etl.git\"\n",
                "\n",
                "if os.path.exists(repo_path):\n",
                "    print(\"   ‚úÖ Repository already exists, pulling latest changes...\")\n",
                "    os.chdir(repo_path)\n",
                "    !git pull\n",
                "else:\n",
                "    print(\"   üì• Cloning from GitHub...\")\n",
                "    !git clone https://github.com/Orshikhbayar/mongolian_history_etl.git\n",
                "    os.chdir(repo_path)\n",
                "\n",
                "# Add to Python path\n",
                "if repo_path not in sys.path:\n",
                "    sys.path.insert(0, repo_path)\n",
                "    print(f\"   ‚úÖ Added {repo_path} to Python path\")\n",
                "\n",
                "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")\n",
                "\n",
                "# Verify stable dataset exists\n",
                "print(\"\\nüìä Checking dataset...\")\n",
                "if os.path.exists('data/mgl_history_grpo_stable.jsonl'):\n",
                "    !ls -lh data/mgl_history_grpo_stable.jsonl\n",
                "    print(\"‚úÖ Stable GRPO dataset found!\")\n",
                "    \n",
                "    # Show dataset stats\n",
                "    with open('data/mgl_history_grpo_stable.jsonl', 'r', encoding='utf-8') as f:\n",
                "        lines = f.readlines()\n",
                "    print(f\"üìà Dataset contains {len(lines)} training pairs\")\n",
                "elif os.path.exists('data/mgl_history_grpo.jsonl'):\n",
                "    !ls -lh data/mgl_history_grpo.jsonl\n",
                "    print(\"‚úÖ GRPO dataset found (using mgl_history_grpo.jsonl)\")\n",
                "    with open('data/mgl_history_grpo.jsonl', 'r', encoding='utf-8') as f:\n",
                "        lines = f.readlines()\n",
                "    print(f\"üìà Dataset contains {len(lines)} training pairs\")\n",
                "else:\n",
                "    print(\"‚ùå GRPO dataset not found. Available datasets:\")\n",
                "    !ls -la data/*.jsonl 2>/dev/null || echo \"No .jsonl files found in data/\"\n",
                "\n",
                "print(\"\\nüéØ Ready for GRPO training!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Generate GRPO Dataset (if needed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "generate-grpo-dataset"
            },
            "outputs": [],
            "source": [
                "#@title üìä Generate Main GRPO Dataset\n",
                "#@markdown Run this cell if the main GRPO dataset doesn't exist\n",
                "openai_api_key_gen = \"\" #@param {type:\"string\"}\n",
                "pairs_target = 100 #@param {type:\"integer\"}\n",
                "generate_dataset = True #@param {type:\"boolean\"}\n",
                "\n",
                "import os\n",
                "from getpass import getpass\n",
                "\n",
                "# Check if main dataset exists\n",
                "main_dataset = 'data/mgl_history_grpo_stable.jsonl'\n",
                "if os.path.exists(main_dataset):\n",
                "    with open(main_dataset, 'r', encoding='utf-8') as f:\n",
                "        lines = f.readlines()\n",
                "    print(f\"‚úÖ Main dataset already exists: {len(lines)} pairs\")\n",
                "    print(\"No need to generate - ready for training!\")\n",
                "else:\n",
                "    print(\"‚ùå Main GRPO dataset not found\")\n",
                "    print(\"Available datasets:\")\n",
                "    !ls -la data/*grpo*.jsonl\n",
                "    \n",
                "    if generate_dataset:\n",
                "        print(\"\\nüîß Generating main GRPO dataset...\")\n",
                "        \n",
                "        # Set API key\n",
                "        if not openai_api_key_gen:\n",
                "            print(\"üîë Enter your OpenAI API key:\")\n",
                "            openai_api_key_gen = getpass(\"OpenAI API Key: \")\n",
                "        \n",
                "        os.environ['OPENAI_API_KEY'] = openai_api_key_gen\n",
                "        \n",
                "        # Generate dataset\n",
                "        print(f\"üöÄ Generating {pairs_target} GRPO pairs...\")\n",
                "        !python scripts/build_grpo_dataset_stable.py --pairs-target {pairs_target} --output {main_dataset}\n",
                "        \n",
                "        # Verify generation\n",
                "        if os.path.exists(main_dataset):\n",
                "            with open(main_dataset, 'r', encoding='utf-8') as f:\n",
                "                lines = f.readlines()\n",
                "            print(f\"‚úÖ Dataset generated successfully: {len(lines)} pairs\")\n",
                "        else:\n",
                "            print(\"‚ùå Dataset generation failed\")\n",
                "    else:\n",
                "        print(\"\\n‚ö†Ô∏è Dataset generation skipped\")\n",
                "        print(\"You can use existing test datasets or enable generation above\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Direct GRPO Training from Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "direct-grpo-training"
            },
            "outputs": [],
            "source": [
                "#@title üöÄ Run GRPO Training on Stable Dataset\n",
                "#@markdown Configure training parameters:\n",
                "openai_api_key = \"\" #@param {type:\"string\"}\n",
                "use_wandb = False #@param {type:\"boolean\"}\n",
                "wandb_api_key = \"\" #@param {type:\"string\"}\n",
                "batch_size = 2 #@param {type:\"integer\"}\n",
                "learning_rate = 5e-6 #@param {type:\"number\"}\n",
                "epochs = 2 #@param {type:\"integer\"}\n",
                "max_length = 512 #@param {type:\"integer\"}\n",
                "lora_r = 16 #@param {type:\"integer\"}\n",
                "lora_alpha = 32 #@param {type:\"integer\"}\n",
                "\n",
                "import os\n",
                "from getpass import getpass\n",
                "\n",
                "# Set API keys\n",
                "if not openai_api_key:\n",
                "    print(\"üîë Enter your OpenAI API key:\")\n",
                "    openai_api_key = getpass(\"OpenAI API Key: \")\n",
                "\n",
                "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
                "\n",
                "if use_wandb and wandb_api_key:\n",
                "    os.environ['WANDB_API_KEY'] = wandb_api_key\n",
                "    !wandb login\n",
                "\n",
                "# Verify we're in the right directory and have the dataset\n",
                "if not os.path.exists('data/mgl_history_grpo_stable.jsonl'):\n",
                "    print(\"‚ùå Stable dataset not found! Please run the setup cell first.\")\n",
                "    exit()\n",
                "\n",
                "# Build training command\n",
                "training_cmd = f\"\"\"\n",
                "python scripts/train_grpo_model.py \\\n",
                "  --base mistralai/Mistral-7B-Instruct-v0.2 \\\n",
                "  --dataset data/mgl_history_grpo_stable.jsonl \\\n",
                "  --output models/mgl_history_grpo_adapter \\\n",
                "  --batch-size {batch_size} \\\n",
                "  --learning-rate {learning_rate} \\\n",
                "  --epochs {epochs} \\\n",
                "  --max-length {max_length} \\\n",
                "  --lora-r {lora_r} \\\n",
                "  --lora-alpha {lora_alpha}\n",
                "\"\"\"\n",
                "\n",
                "if use_wandb and wandb_api_key:\n",
                "    training_cmd += \" --use-wandb\"\n",
                "\n",
                "print(\"üî• Starting GRPO training with stable dataset...\")\n",
                "print(f\"üìä Dataset: data/mgl_history_grpo_stable.jsonl\")\n",
                "print(f\"ü§ñ Model: mistralai/Mistral-7B-Instruct-v0.2\")\n",
                "print(f\"üíæ Output: models/mgl_history_grpo_adapter\")\n",
                "print(f\"‚öôÔ∏è Batch size: {batch_size}, LR: {learning_rate}, Epochs: {epochs}\")\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "\n",
                "# Execute training\n",
                "!{training_cmd}\n",
                "\n",
                "print(\"\\nüéâ Training completed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà Training Results and Model Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "results-testing"
            },
            "outputs": [],
            "source": [
                "#@title üìà View Results and Test Model\n",
                "#@markdown This cell will:\n",
                "#@markdown - Display training statistics\n",
                "#@markdown - Load the trained model\n",
                "#@markdown - Test with Mongolian historical questions\n",
                "#@markdown - Create downloadable model archive\n",
                "\n",
                "import json\n",
                "import os\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "from peft import PeftModel\n",
                "import zipfile\n",
                "from datetime import datetime\n",
                "from google.colab import files\n",
                "\n",
                "# Display training statistics\n",
                "print(\"üìä TRAINING RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "stats_file = 'models/mgl_history_grpo_adapter/training_logs/training_stats.json'\n",
                "if os.path.exists(stats_file):\n",
                "    with open(stats_file, 'r') as f:\n",
                "        stats = json.load(f)\n",
                "    \n",
                "    print(f\"‚úÖ Base model: {stats.get('base_model', 'mistralai/Mistral-7B-Instruct-v0.2')}\")\n",
                "    print(f\"üìä Dataset: data/mgl_history_grpo_stable.jsonl\")\n",
                "    print(f\"üî¢ Training samples: {stats.get('train_samples', 'N/A')}\")\n",
                "    print(f\"üî¢ Test samples: {stats.get('test_samples', 'N/A')}\")\n",
                "    print(f\"üîÑ Epochs completed: {stats.get('epochs', 'N/A')}\")\n",
                "    print(f\"üìâ Final loss: {stats.get('final_loss', 'N/A'):.4f}\")\n",
                "    print(f\"üéØ Validation accuracy: {stats.get('validation_accuracy', 'N/A'):.1%}\")\n",
                "    print(f\"üèÜ Mean reward: {stats.get('mean_reward', 'N/A'):.3f}\")\n",
                "    print(f\"‚è±Ô∏è Training time: {stats.get('training_time', 'N/A'):.1f}s\")\n",
                "    print(f\"üíæ Model size: {stats.get('model_size_mb', 'N/A'):.1f}MB\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Training statistics not found.\")\n",
                "    print(\"Available files:\")\n",
                "    !find models/ -name \"*.json\" -type f\n",
                "\n",
                "# Test the trained model\n",
                "print(\"\\nüß™ TESTING TRAINED MODEL\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "if os.path.exists('models/mgl_history_grpo_adapter'):\n",
                "    try:\n",
                "        print(\"üì• Loading trained model...\")\n",
                "        \n",
                "        # Load base model and tokenizer\n",
                "        base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
                "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
                "        base_model = AutoModelForCausalLM.from_pretrained(\n",
                "            base_model_name,\n",
                "            torch_dtype=torch.float16,\n",
                "            device_map=\"auto\",\n",
                "            load_in_8bit=True  # Use 8-bit to save memory\n",
                "        )\n",
                "        \n",
                "        # Load LoRA adapter\n",
                "        model = PeftModel.from_pretrained(base_model, 'models/mgl_history_grpo_adapter')\n",
                "        \n",
                "        if tokenizer.pad_token is None:\n",
                "            tokenizer.pad_token = tokenizer.eos_token\n",
                "        \n",
                "        print(\"‚úÖ Model loaded successfully!\")\n",
                "        \n",
                "        # Test with Mongolian historical questions\n",
                "        test_questions = [\n",
                "            \"–ß–∏–Ω–≥–∏—Å —Ö–∞–∞–Ω—ã —Ç—É—Ö–∞–π —Ç–æ–≤—á —è—Ä–∏–Ω–∞ —É—É?\",\n",
                "            \"1921 –æ–Ω—ã —Ö—É–≤—å—Å–≥–∞–ª—ã–Ω “Ø—Ä –¥“Ø–Ω —é—É –±–∞–π—Å–∞–Ω –±—ç?\",\n",
                "            \"–ú–æ–Ω–≥–æ–ª—ã–Ω –∞—Ä–¥—á–∏–ª—Å–∞–Ω —Ö—É–≤—å—Å–≥–∞–ª —Ö—ç–∑—ç—ç –±–æ–ª—Å–æ–Ω –±—ç?\",\n",
                "            \"–ë–æ–≥–¥ —Ö–∞–∞–Ω—ã “Ø–µ–∏–π–Ω –æ–Ω—Ü–ª–æ–≥ —Ç—É—Ö–∞–π —Ö—ç–ª–Ω—ç “Ø“Ø?\"\n",
                "        ]\n",
                "        \n",
                "        for i, question in enumerate(test_questions, 1):\n",
                "            print(f\"\\n{i}. ‚ùì {question}\")\n",
                "            \n",
                "            # Format prompt\n",
                "            formatted_prompt = f\"<|user|>\\n{question}\\n<|assistant|>\\n\"\n",
                "            \n",
                "            # Tokenize\n",
                "            inputs = tokenizer(\n",
                "                formatted_prompt,\n",
                "                return_tensors=\"pt\",\n",
                "                truncation=True,\n",
                "                max_length=512\n",
                "            )\n",
                "            \n",
                "            if torch.cuda.is_available():\n",
                "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
                "            \n",
                "            # Generate response\n",
                "            with torch.no_grad():\n",
                "                outputs = model.generate(\n",
                "                    **inputs,\n",
                "                    max_new_tokens=200,\n",
                "                    do_sample=True,\n",
                "                    temperature=0.7,\n",
                "                    top_p=0.9,\n",
                "                    pad_token_id=tokenizer.eos_token_id\n",
                "                )\n",
                "            \n",
                "            # Decode response\n",
                "            response = tokenizer.decode(\n",
                "                outputs[0][inputs['input_ids'].shape[1]:],\n",
                "                skip_special_tokens=True\n",
                "            ).strip()\n",
                "            \n",
                "            print(f\"   ü§ñ {response}\")\n",
                "        \n",
                "        # Clean up memory\n",
                "        del model, base_model, tokenizer\n",
                "        torch.cuda.empty_cache()\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error testing model: {e}\")\n",
                "        print(\"This might be due to memory constraints.\")\n",
                "\n",
                "# Create downloadable archive\n",
                "print(\"\\nüì¶ CREATING DOWNLOAD ARCHIVE\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "if os.path.exists('models/mgl_history_grpo_adapter'):\n",
                "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "    archive_name = f\"mongolian_grpo_model_{timestamp}.zip\"\n",
                "    \n",
                "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
                "        # Add model files\n",
                "        for root, dirs, files in os.walk('models/mgl_history_grpo_adapter'):\n",
                "            for file in files:\n",
                "                file_path = os.path.join(root, file)\n",
                "                arcname = os.path.relpath(file_path, '.')\n",
                "                zipf.write(file_path, arcname)\n",
                "        \n",
                "        # Add stable dataset\n",
                "        if os.path.exists('data/mgl_history_grpo_stable.jsonl'):\n",
                "            zipf.write('data/mgl_history_grpo_stable.jsonl', 'data/mgl_history_grpo_stable.jsonl')\n",
                "        \n",
                "        # Add stats if available\n",
                "        if os.path.exists('data/mgl_history_grpo_stats_stable.json'):\n",
                "            zipf.write('data/mgl_history_grpo_stats_stable.json', 'data/mgl_history_grpo_stats_stable.json')\n",
                "    \n",
                "    print(f\"‚úÖ Archive created: {archive_name}\")\n",
                "    print(f\"üìÅ Size: {os.path.getsize(archive_name) / 1024 / 1024:.1f} MB\")\n",
                "    \n",
                "    # Download\n",
                "    print(\"‚¨áÔ∏è Starting download...\")\n",
                "    files.download(archive_name)\n",
                "    \n",
                "    print(\"\\nüéâ SUCCESS! Your trained Mongolian GRPO model is ready!\")\n",
                "    print(\"\\nüìã Archive contains:\")\n",
                "    print(\"   ‚úÖ Trained LoRA adapter\")\n",
                "    print(\"   ‚úÖ Training logs and statistics\")\n",
                "    print(\"   ‚úÖ Stable GRPO dataset\")\n",
                "    print(\"   ‚úÖ Dataset statistics\")\n",
                "    \n",
                "    print(\"\\nüöÄ Integration example:\")\n",
                "    print(\"```python\")\n",
                "    print(\"from peft import PeftModel\")\n",
                "    print(\"from transformers import AutoModelForCausalLM\")\n",
                "    print(\"\")\n",
                "    print(\"base_model = AutoModelForCausalLM.from_pretrained(\")\n",
                "    print(\"    'mistralai/Mistral-7B-Instruct-v0.2'\")\n",
                "    print(\"model = PeftModel.from_pretrained(\")\n",
                "    print(\"    base_model, 'models/mgl_history_grpo_adapter'\")\n",
                "    print(\")\")\n",
                "    print(\"```\")\n",
                "    \n",
                "else:\n",
                "    print(\"‚ùå No trained model found to archive.\")\n",
                "\n",
                "print(\"\\nüåü Training complete! Your Mongolian historical AI is ready to use.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîÑ Push Results Back to GitHub (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "push-to-github"
            },
            "outputs": [],
            "source": [
                "#@title üîÑ Push Trained Model to GitHub\n",
                "#@markdown This will commit and push your trained model back to your repository\n",
                "push_to_github = False #@param {type:\"boolean\"}\n",
                "github_token = \"\" #@param {type:\"string\"}\n",
                "commit_message = \"Add trained GRPO model from Colab\" #@param {type:\"string\"}\n",
                "\n",
                "import os\n",
                "from getpass import getpass\n",
                "\n",
                "if push_to_github:\n",
                "    print(\"üîß Setting up Git credentials...\")\n",
                "    \n",
                "    # Get GitHub token if not provided\n",
                "    if not github_token:\n",
                "        print(\"üîë You need a GitHub Personal Access Token\")\n",
                "        print(\"Create one at: https://github.com/settings/tokens\")\n",
                "        print(\"Required permissions: repo (full control)\")\n",
                "        github_token = getpass(\"GitHub Token: \")\n",
                "    \n",
                "    # Configure Git\n",
                "    !git config --global user.name \"Colab Training Bot\"\n",
                "    !git config --global user.email \"colab@training.bot\"\n",
                "    \n",
                "    # Set remote URL with token\n",
                "    !git remote set-url origin https://{github_token}@github.com/Orshikhbayar/mongolian_history_etl.git\n",
                "    \n",
                "    # Check what files to add\n",
                "    print(\"\\nüìÅ Files to commit:\")\n",
                "    !git status --porcelain\n",
                "    \n",
                "    # Add trained model files\n",
                "    if os.path.exists('models/mgl_history_grpo_adapter'):\n",
                "        !git add models/mgl_history_grpo_adapter/\n",
                "        print(\"‚úÖ Added trained model\")\n",
                "    \n",
                "    # Add any generated datasets\n",
                "    if os.path.exists('data/mgl_history_grpo_stable.jsonl'):\n",
                "        !git add data/mgl_history_grpo_stable.jsonl\n",
                "        print(\"‚úÖ Added generated dataset\")\n",
                "    \n",
                "    # Add training logs\n",
                "    !git add -A training_logs/ 2>/dev/null || echo \"No training logs to add\"\n",
                "    \n",
                "    # Commit changes\n",
                "    try:\n",
                "        !git commit -m \"{commit_message}\"\n",
                "        print(\"‚úÖ Changes committed\")\n",
                "        \n",
                "        # Push to GitHub\n",
                "        !git push origin main\n",
                "        print(\"üöÄ Successfully pushed to GitHub!\")\n",
                "        print(\"\\nüéØ To sync in Kiro, run: git pull origin main\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error during commit/push: {e}\")\n",
                "        print(\"You can still download the model archive manually\")\n",
                "        \n",
                "else:\n",
                "    print(\"‚è≠Ô∏è Skipping GitHub push\")\n",
                "    print(\"\\nüí° To push later:\")\n",
                "    print(\"1. Enable 'push_to_github' above\")\n",
                "    print(\"2. Get GitHub token from: https://github.com/settings/tokens\")\n",
                "    print(\"3. Re-run this cell\")\n",
                "    print(\"\\nüì• Or download the model archive and sync manually\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìÅ Upload Project Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "upload-project"
            },
            "outputs": [],
            "source": [
                "# Option 1: Upload from local files\n",
                "from google.colab import files\n",
                "import zipfile\n",
                "import os\n",
                "\n",
                "print(\"üì§ Upload your project as a ZIP file:\")\n",
                "print(\"1. Zip your entire project folder\")\n",
                "print(\"2. Upload it using the file browser below\")\n",
                "print(\"3. The notebook will extract it automatically\")\n",
                "\n",
                "# Upload ZIP file\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Extract the first ZIP file found\n",
                "for filename in uploaded.keys():\n",
                "    if filename.endswith('.zip'):\n",
                "        print(f\"üì¶ Extracting {filename}...\")\n",
                "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
                "            zip_ref.extractall('.')\n",
                "        print(\"‚úÖ Project extracted successfully!\")\n",
                "        break\n",
                "\n",
                "# List project structure\n",
                "!ls -la"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "alternative-setup"
            },
            "outputs": [],
            "source": [
                "# Option 2: Clone from GitHub (if you've pushed your project)\n",
                "# Uncomment and modify the following lines:\n",
                "\n",
                "# !git clone https://github.com/yourusername/mongolian-history-project.git\n",
                "# %cd mongolian-history-project\n",
                "# !ls -la"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîë Set API Keys"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "set-api-keys"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "from getpass import getpass\n",
                "\n",
                "# Set OpenAI API key\n",
                "print(\"üîë Enter your OpenAI API key:\")\n",
                "openai_key = getpass(\"OpenAI API Key: \")\n",
                "os.environ['OPENAI_API_KEY'] = openai_key\n",
                "\n",
                "# Optional: Set Weights & Biases key for experiment tracking\n",
                "print(\"\\nüîë Enter your W&B API key (optional, press Enter to skip):\")\n",
                "wandb_key = getpass(\"W&B API Key (optional): \")\n",
                "if wandb_key:\n",
                "    os.environ['WANDB_API_KEY'] = wandb_key\n",
                "    !wandb login\n",
                "\n",
                "print(\"‚úÖ API keys configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Generate GRPO Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "generate-grpo-dataset"
            },
            "outputs": [],
            "source": [
                "# Check if we have existing datasets\n",
                "!ls -la data/\n",
                "\n",
                "# Generate GRPO dataset if not exists\n",
                "import os\n",
                "if not os.path.exists('data/mgl_history_grpo.jsonl'):\n",
                "    print(\"üìä Generating GRPO dataset...\")\n",
                "    !python scripts/build_grpo_dataset.py --pairs-per-topic 20 --output data/mgl_history_grpo.jsonl\n",
                "else:\n",
                "    print(\"‚úÖ GRPO dataset already exists\")\n",
                "\n",
                "# Validate the dataset\n",
                "!python scripts/validate_mgl_dataset.py --files data/mgl_history_grpo.jsonl"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Run GRPO Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run-grpo-training"
            },
            "outputs": [],
            "source": [
                "# Start GRPO training with optimized settings for Colab\n",
                "training_command = \"\"\"\n",
                "python scripts/train_grpo_model.py \\\n",
                "  --base mistralai/Mistral-7B-Instruct-v0.2 \\\n",
                "  --dataset data/mgl_history_grpo.jsonl \\\n",
                "  --output models/mgl_history_grpo_adapter \\\n",
                "  --batch-size 2 \\\n",
                "  --learning-rate 5e-6 \\\n",
                "  --epochs 2 \\\n",
                "  --max-length 512 \\\n",
                "  --lora-r 16 \\\n",
                "  --lora-alpha 32\n",
                "\"\"\"\n",
                "\n",
                "# Add W&B logging if key is set\n",
                "if 'WANDB_API_KEY' in os.environ:\n",
                "    training_command += \" --use-wandb\"\n",
                "\n",
                "print(\"üî• Starting GRPO training...\")\n",
                "print(f\"Command: {training_command.strip()}\")\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "\n",
                "# Execute training\n",
                "!{training_command}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà Monitor Training Progress"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "monitor-training"
            },
            "outputs": [],
            "source": [
                "# Check training logs\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Display training statistics\n",
                "if os.path.exists('training_logs/training_stats.json'):\n",
                "    with open('training_logs/training_stats.json', 'r') as f:\n",
                "        stats = json.load(f)\n",
                "    \n",
                "    print(\"üìä TRAINING RESULTS\")\n",
                "    print(\"=\" * 30)\n",
                "    print(f\"Base model: {stats.get('base_model', 'N/A')}\")\n",
                "    print(f\"Dataset samples: {stats.get('total_samples', 'N/A')}\")\n",
                "    print(f\"Training samples: {stats.get('train_samples', 'N/A')}\")\n",
                "    print(f\"Test samples: {stats.get('test_samples', 'N/A')}\")\n",
                "    print(f\"Epochs: {stats.get('epochs', 'N/A')}\")\n",
                "    print(f\"Total steps: {stats.get('total_steps', 'N/A')}\")\n",
                "    print(f\"Average loss: {stats.get('avg_loss', 'N/A'):.4f}\")\n",
                "    print(f\"Final loss: {stats.get('final_loss', 'N/A'):.4f}\")\n",
                "    print(f\"Mean reward: {stats.get('mean_reward', 'N/A'):.3f}\")\n",
                "    print(f\"Validation accuracy: {stats.get('validation_accuracy', 'N/A'):.1%}\")\n",
                "    print(f\"Training time: {stats.get('training_time', 'N/A'):.1f}s\")\n",
                "    print(f\"Model size: {stats.get('model_size_mb', 'N/A'):.1f}MB\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Training statistics not found. Check if training completed successfully.\")\n",
                "\n",
                "# Check model files\n",
                "print(\"\\nüìÅ MODEL FILES:\")\n",
                "!ls -la models/mgl_history_grpo_adapter/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ Test Trained Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test-trained-model"
            },
            "outputs": [],
            "source": [
                "# Load and test the trained model\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "from peft import PeftModel\n",
                "import torch\n",
                "\n",
                "def load_trained_model():\n",
                "    \"\"\"Load the trained GRPO model.\"\"\"\n",
                "    base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
                "    adapter_path = \"models/mgl_history_grpo_adapter\"\n",
                "    \n",
                "    print(\"üì• Loading base model...\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        base_model_name,\n",
                "        torch_dtype=torch.float16,\n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    \n",
                "    print(\"üîß Loading LoRA adapter...\")\n",
                "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
                "    \n",
                "    if tokenizer.pad_token is None:\n",
                "        tokenizer.pad_token = tokenizer.eos_token\n",
                "    \n",
                "    return model, tokenizer\n",
                "\n",
                "def generate_response(model, tokenizer, prompt, max_length=256):\n",
                "    \"\"\"Generate response using the trained model.\"\"\"\n",
                "    formatted_prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
                "    \n",
                "    inputs = tokenizer(\n",
                "        formatted_prompt,\n",
                "        return_tensors=\"pt\",\n",
                "        truncation=True,\n",
                "        max_length=512\n",
                "    )\n",
                "    \n",
                "    if torch.cuda.is_available():\n",
                "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_length,\n",
                "            do_sample=True,\n",
                "            temperature=0.7,\n",
                "            top_p=0.9,\n",
                "            pad_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(\n",
                "        outputs[0][inputs['input_ids'].shape[1]:],\n",
                "        skip_special_tokens=True\n",
                "    ).strip()\n",
                "    \n",
                "    return response\n",
                "\n",
                "# Load the trained model\n",
                "if os.path.exists('models/mgl_history_grpo_adapter'):\n",
                "    try:\n",
                "        model, tokenizer = load_trained_model()\n",
                "        print(\"‚úÖ Model loaded successfully!\")\n",
                "        \n",
                "        # Test with sample prompts\n",
                "        test_prompts = [\n",
                "            \"–ß–∏–Ω–≥–∏—Å —Ö–∞–∞–Ω—ã —Ç—É—Ö–∞–π —è—Ä–∏–Ω–∞ —É—É?\",\n",
                "            \"1921 –æ–Ω—ã —Ö—É–≤—å—Å–≥–∞–ª—ã–Ω “Ø—Ä –¥“Ø–Ω —é—É –±–∞–π—Å–∞–Ω –±—ç?\",\n",
                "            \"–ú–æ–Ω–≥–æ–ª—ã–Ω –∞—Ä–¥—á–∏–ª—Å–∞–Ω —Ö—É–≤—å—Å–≥–∞–ª —Ö—ç—Ä—Ö—ç–Ω ”©—Ä–Ω”©—Å”©–Ω –±—ç?\",\n",
                "            \"–ë–æ–≥–¥ —Ö–∞–∞–Ω—ã “Ø–µ–∏–π–Ω –æ–Ω—Ü–ª–æ–≥ –Ω—å —é—É –≤—ç?\"\n",
                "        ]\n",
                "        \n",
                "        print(\"\\nüß™ TESTING TRAINED MODEL\")\n",
                "        print(\"=\" * 40)\n",
                "        \n",
                "        for i, prompt in enumerate(test_prompts, 1):\n",
                "            print(f\"\\n{i}. ‚ùì {prompt}\")\n",
                "            response = generate_response(model, tokenizer, prompt)\n",
                "            print(f\"   ü§ñ {response}\")\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error loading model: {e}\")\n",
                "        print(\"This might be due to memory constraints or incomplete training.\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Trained model not found. Make sure training completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üíæ Download Trained Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download-model"
            },
            "outputs": [],
            "source": [
                "# Create a ZIP file with the trained model and logs\n",
                "import zipfile\n",
                "import os\n",
                "from datetime import datetime\n",
                "\n",
                "def create_model_archive():\n",
                "    \"\"\"Create a ZIP archive with trained model and logs.\"\"\"\n",
                "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "    archive_name = f\"mongolian_grpo_model_{timestamp}.zip\"\n",
                "    \n",
                "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
                "        # Add model files\n",
                "        if os.path.exists('models/mgl_history_grpo_adapter'):\n",
                "            for root, dirs, files in os.walk('models/mgl_history_grpo_adapter'):\n",
                "                for file in files:\n",
                "                    file_path = os.path.join(root, file)\n",
                "                    arcname = os.path.relpath(file_path, '.')\n",
                "                    zipf.write(file_path, arcname)\n",
                "        \n",
                "        # Add training logs\n",
                "        if os.path.exists('training_logs'):\n",
                "            for root, dirs, files in os.walk('training_logs'):\n",
                "                for file in files:\n",
                "                    file_path = os.path.join(root, file)\n",
                "                    arcname = os.path.relpath(file_path, '.')\n",
                "                    zipf.write(file_path, arcname)\n",
                "        \n",
                "        # Add dataset if exists\n",
                "        if os.path.exists('data/mgl_history_grpo.jsonl'):\n",
                "            zipf.write('data/mgl_history_grpo.jsonl', 'data/mgl_history_grpo.jsonl')\n",
                "    \n",
                "    return archive_name\n",
                "\n",
                "# Create and download the archive\n",
                "if os.path.exists('models/mgl_history_grpo_adapter'):\n",
                "    print(\"üì¶ Creating model archive...\")\n",
                "    archive_name = create_model_archive()\n",
                "    \n",
                "    print(f\"‚úÖ Archive created: {archive_name}\")\n",
                "    print(f\"üìÅ Archive size: {os.path.getsize(archive_name) / 1024 / 1024:.1f} MB\")\n",
                "    \n",
                "    # Download the archive\n",
                "    print(\"‚¨áÔ∏è Downloading archive...\")\n",
                "    files.download(archive_name)\n",
                "    \n",
                "    print(\"\\nüéâ SUCCESS! Your trained model is ready!\")\n",
                "    print(\"\\nüìã What you got:\")\n",
                "    print(\"‚úÖ Trained LoRA adapter (models/mgl_history_grpo_adapter/)\")\n",
                "    print(\"‚úÖ Training logs and statistics (training_logs/)\")\n",
                "    print(\"‚úÖ GRPO dataset (data/mgl_history_grpo.jsonl)\")\n",
                "    \n",
                "    print(\"\\nüöÄ Next steps:\")\n",
                "    print(\"1. Extract the ZIP file in your local project\")\n",
                "    print(\"2. Load the model with: PeftModel.from_pretrained(base_model, 'models/mgl_history_grpo_adapter')\")\n",
                "    print(\"3. Integrate with your RAG system for improved Mongolian responses\")\n",
                "    \n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No trained model found to download.\")\n",
                "    print(\"Make sure the training completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Training Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training-summary"
            },
            "outputs": [],
            "source": [
                "# Display final training summary\n",
                "print(\"üéâ MONGOLIAN GRPO TRAINING COMPLETE!\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Check GPU usage\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"üî• GPU Used: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"üíæ VRAM Used: {torch.cuda.memory_allocated(0) / 1024**3:.1f} GB\")\n",
                "\n",
                "# Check training results\n",
                "if os.path.exists('training_logs/training_stats.json'):\n",
                "    with open('training_logs/training_stats.json', 'r') as f:\n",
                "        stats = json.load(f)\n",
                "    \n",
                "    print(f\"\\nüìà Training Results:\")\n",
                "    print(f\"   ‚Ä¢ Final Loss: {stats.get('final_loss', 'N/A'):.4f}\")\n",
                "    print(f\"   ‚Ä¢ Validation Accuracy: {stats.get('validation_accuracy', 'N/A'):.1%}\")\n",
                "    print(f\"   ‚Ä¢ Mean Reward: {stats.get('mean_reward', 'N/A'):.3f}\")\n",
                "    print(f\"   ‚Ä¢ Training Time: {stats.get('training_time', 'N/A'):.1f}s\")\n",
                "\n",
                "print(\"\\n‚úÖ Your Mongolian historical AI model is now trained and ready!\")\n",
                "print(\"\\nüîÆ The model can now provide more accurate, culturally appropriate,\")\n",
                "print(\"   and historically informed responses in Mongolian language.\")\n",
                "\n",
                "print(\"\\nüéØ Integration Example:\")\n",
                "print(\"```python\")\n",
                "print(\"from peft import PeftModel\")\n",
                "print(\"from transformers import AutoModelForCausalLM\")\n",
                "print(\"\")\n",
                "print(\"base_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2')\")\n",
                "print(\"model = PeftModel.from_pretrained(base_model, 'models/mgl_history_grpo_adapter')\")\n",
                "print(\"```\")\n",
                "\n",
                "print(\"\\nüåü Congratulations on successfully training your Mongolian AI model!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
